{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-YmKKWySJkz",
        "outputId": "022a3cad-38ef-4034-b22b-3c186dbc5d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary module and dependencies\n",
        "%pip install -q \"git+https://github.com/openai/whisper.git\" \"gradio\" \"openai\" \"gTTS\" \"torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j59YxO0jI0Vd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/grammonde/miniconda3/envs/voice-assistant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-rwxrdzqf because the default path (/home/grammonde/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import torch\n",
        "import gradio as gr\n",
        "import openai\n",
        "from gtts import gTTS\n",
        "from voicevox_client import run\n",
        "from constants import KEYS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdGGLqrNGZTz",
        "outputId": "597aab58-899b-4fb4-99e3-9488da53b72a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, lavfi, from 'anullsrc=r=44100:cl=mono':\n",
            "  Duration: N/A, start: 0.000000, bitrate: 352 kb/s\n",
            "  Stream #0:0: Audio: pcm_u8, 44100 Hz, mono, u8, 352 kb/s\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (pcm_u8 (native) -> mp3 (libmp3lame))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp3, to 'audio_response.mp3':\n",
            "  Metadata:\n",
            "    TSSE            : Lavf58.76.100\n",
            "  Stream #0:0: Audio: mp3, 44100 Hz, mono, s16p\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 libmp3lame\n",
            "size=      39kB time=00:00:09.97 bitrate=  32.2kbits/s speed= 462x    \n",
            "video:0kB audio:39kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.568409%\n"
          ]
        }
      ],
      "source": [
        "# Generate an mp3 file\n",
        "!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame audio_response.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1GCU3_VzaTjQ"
      },
      "outputs": [],
      "source": [
        "# Set up device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BWjxk0FtZLHY"
      },
      "outputs": [],
      "source": [
        "# Load openai api key\n",
        "openai.api_key = KEYS['OPEN_AI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mdCBQLGYwUQ",
        "outputId": "a0558c80-f624-4964-8bb5-d2ea9c4be3a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load base model of the whisper\n",
        "\n",
        "model = whisper.load_model('base')\n",
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fXJM3JMiZs44"
      },
      "outputs": [],
      "source": [
        "# Define a function to pass the input to chatGPT api\n",
        "def chat_completion_gpt(text: str):\n",
        "  messages = [{'role': 'user', 'content': text }] if text else [{ 'role': 'system', 'content': 'Goodbye have a nice day' }]\n",
        "\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = messages,\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uEslfMFPAau-"
      },
      "outputs": [],
      "source": [
        "def text_voice(input_text, language = 'en'):\n",
        "  generated_audio = gTTS(\n",
        "    text = input_text,\n",
        "    lang = language,\n",
        "    slow = False,\n",
        "  )\n",
        "\n",
        "  generated_audio.save('audio_response.mp3')\n",
        "\n",
        "  return 'audio_response.mp3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3MsQvn3R1Ywl"
      },
      "outputs": [],
      "source": [
        "# Define a function to transcribe the \n",
        "def transcribe_audio(audio):\n",
        "\n",
        "  # Load the audio and trim it to fit 30s\n",
        "  audio = whisper.load_audio(audio)\n",
        "  audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "  # make log-Mel spectrogram and move to the same device as the model\n",
        "  mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "  # Detect the language\n",
        "  _, probs = model.detect_language(mel)\n",
        "  print(f'Detected Language: {max(probs, key = probs.get)}')\n",
        "  language = max(probs, key = probs.get)\n",
        "\n",
        "  # Decode the the audio\n",
        "  options = whisper.DecodingOptions()\n",
        "  decode_obj = whisper.decode(model, mel, options)\n",
        "  text = decode_obj.text\n",
        "  \n",
        "  # Pass the result to the chatGPT\n",
        "  response = chat_completion_gpt(text)\n",
        "\n",
        "  # Call the text_to_voice function\n",
        "  result = run(input_text = response) if language == 'ja' else text_voice(input_text = response, language = language)\n",
        "\n",
        "  return [text, response, result]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "mRx9pPv39fEX",
        "outputId": "6fd1cad6-b71d-40dc-c463-cedf15049dbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/grammonde/miniconda3/envs/voice-assistant/lib/python3.11/site-packages/gradio/inputs.py:321: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/home/grammonde/miniconda3/envs/voice-assistant/lib/python3.11/site-packages/gradio/inputs.py:324: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  super().__init__(source=source, type=type, label=label, optional=optional)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://61fd85ba69731c0c48.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://61fd85ba69731c0c48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/grammonde/miniconda3/envs/voice-assistant/lib/python3.11/site-packages/gradio/processing_utils.py:176: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected Language: en\n"
          ]
        }
      ],
      "source": [
        "# Run with gradio interface\n",
        "\n",
        "speech_text = gr.Textbox(label = 'Speech to text')\n",
        "response_text = gr.Textbox(label = 'Response in text')\n",
        "text_speech = gr.Audio('audio_response.mp3')\n",
        "\n",
        "gr.Interface(\n",
        "  title = 'Voice assistant with ChatGPT Whisper and Google Voice Generation',\n",
        "  fn = transcribe_audio,\n",
        "  inputs = [gr.inputs.Audio(source = 'microphone', type = 'filepath')],\n",
        "  outputs = [\n",
        "    speech_text,\n",
        "    response_text,\n",
        "    text_speech,\n",
        "  ],\n",
        "  live = True,\n",
        ").launch(share = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "voice-assistant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
